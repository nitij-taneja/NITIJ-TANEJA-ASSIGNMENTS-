{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
        "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
        "probability that an employee is a smoker given that he/she uses the health insurance plan?"
      ],
      "metadata": {
        "id": "waYTUVk2oaDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The probability of an employee being a smoker given that they use the health insurance plan can be calculated using conditional probability. It is the probability of being a smoker and using the plan divided by the probability of using the plan.\n",
        "\n",
        "So, P(Smoker | Uses Plan) = P(Smoker and Uses Plan) / P(Uses Plan)\n",
        "\n",
        "P(Smoker | Uses Plan) = 0.40 / 0.70 = 4/7 or approximately 0.57.\n",
        "\n",
        "Therefore, the probability that an employee is a smoker given that they use the health insurance plan is approximately 0.57."
      ],
      "metadata": {
        "id": "QEOAIzZFq4Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n"
      ],
      "metadata": {
        "id": "-Q7K0ptuodB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Naive Bayes is a classification model that assumes that each feature is independent of each other and that each feature can only take on two values: 0 or 1. Multinomial Naive Bayes is a classification model that assumes that each feature is independent of each other and that each feature can take on any value.\n",
        "\n",
        "In other words, Bernoulli Naive Bayes is a simplified version of Multinomial Naive Bayes that is only applicable to binary features. Multinomial Naive Bayes is more general and can be used for any type of feature.\n",
        "\n",
        "Here is a table that summarizes the key differences between Bernoulli Naive Bayes and Multinomial Naive Bayes:\n",
        "\n",
        "| Feature | Bernoulli Naive Bayes | Multinomial Naive Bayes |\n",
        "|---|---|---|\n",
        "| Number of classes | 2 | Multiple |\n",
        "| Type of features | Binary | Categorical |\n",
        "| Independence assumption | Features are independent | Features are independent |\n",
        "| Conditional probability | P(x_i | y) | P(x_i | y) |\n",
        "| Prior probability | P(y) | P(y) |\n",
        "\n",
        "In general, Multinomial Naive Bayes is more powerful than Bernoulli Naive Bayes because it can model more complex data. However, Bernoulli Naive Bayes is often used when the data is sparse or when the features are not very informative."
      ],
      "metadata": {
        "id": "bq0rtF79ro_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does Bernoulli Naive Bayes handle missing values?"
      ],
      "metadata": {
        "id": "w4MwasLpogg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Naive Bayes handles missing values by considering them as a specific category or a separate state of the feature. In the context of Bernoulli Naive Bayes, which is typically used for binary data, the missing values can be treated as a third category representing the absence of the binary feature.\n",
        "\n",
        "For instance, if your features are binary (0 or 1), and you encounter a missing value, Bernoulli Naive Bayes can treat the missing value as a third category, allowing the algorithm to still calculate probabilities based on the available information.\n",
        "\n",
        "It's important to note that the way missing values are handled can impact the performance of the classifier, and it's advisable to preprocess your data carefully to make informed decisions about how to treat missing values based on the characteristics of your dataset."
      ],
      "metadata": {
        "id": "W7RzzqQps89x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
      ],
      "metadata": {
        "id": "5DvlGX0rojzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. It's an extension of the Naive Bayes algorithm that assumes the features are continuous and follow a Gaussian distribution. For multi-class classification, it calculates the probability of an instance belonging to each class and assigns the class with the highest probability.\n",
        "\n",
        "It works well when the features have a Gaussian (normal) distribution within each class. If your data violates the normality assumption, other variations of Naive Bayes or different classifiers might be more suitable."
      ],
      "metadata": {
        "id": "LPG-xCwes4Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Assignment:\n",
        "\n",
        "Data preparation:\n",
        "\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
        "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
        "is spam or not based on several input features.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
        "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
        "dataset. You should use the default hyperparameters for each classifier.\n",
        "\n",
        "Results:\n",
        "Report the following performance metrics for each classifier:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1 score\n",
        "\n",
        "Discussion:\n",
        "\n",
        "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
        "the case? Are there any limitations of Naive Bayes that you observed?\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Summarise your findings and provide some suggestions for future work.\n",
        "\n",
        "\n",
        "Note: Create your assignment in Jupyter notebook and upload it to GitHub & share that github repository\n",
        "link through your dashboard. Make sure the repository is public.\n",
        "\n",
        "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
        "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
        "Bayes on a real-world problem."
      ],
      "metadata": {
        "id": "e02_uzbVourA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('spambase.data', header=None)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[57]), data[57], test_size=0.25)\n",
        "\n",
        "# Create the classifiers\n",
        "bnb = BernoulliNB()\n",
        "mnb = MultinomialNB()\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifiers\n",
        "bnb.fit(X_train, y_train)\n",
        "mnb.fit(X_train, y_train)\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_bnb = bnb.predict(X_test)\n",
        "y_pred_mnb = mnb.predict(X_test)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifiers\n",
        "accuracy_bnb = accuracy_score(y_test, y_pred_bnb)\n",
        "precision_bnb = precision_score(y_test, y_pred_bnb)\n",
        "recall_bnb = recall_score(y_test, y_pred_bnb)\n",
        "f1_score_bnb = f1_score(y_test, y_pred_bnb)\n",
        "\n",
        "accuracy_mnb = accuracy_score(y_test, y_pred_mnb)\n",
        "precision_mnb = precision_score(y_test, y_pred_mnb)\n",
        "recall_mnb = recall_score(y_test, y_pred_mnb)\n",
        "f1_score_mnb = f1_score(y_test, y_pred_mnb)\n",
        "\n",
        "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
        "precision_gnb = precision_score(y_test, y_pred_gnb)\n",
        "recall_gnb = recall_score(y_test, y_pred_gnb)\n",
        "f1_score_gnb = f1_score(y_test, y_pred_gnb)\n",
        "\n",
        "# Print the results\n",
        "print('Bernoulli Naive Bayes:')\n",
        "print('Accuracy:', accuracy_bnb)\n",
        "print('Precision:', precision_bnb)\n",
        "print('Recall:', recall_bnb)\n",
        "print('F1 score:', f1_score_bnb)\n",
        "\n",
        "print('Multinomial Naive Bayes:')\n",
        "print('Accuracy:', accuracy_mnb)\n",
        "print('Precision:', precision_mnb)\n",
        "print('Recall:', recall_mnb)\n",
        "print('F1 score:', f1_score_mnb)\n",
        "\n",
        "print('Gaussian Naive Bayes:')\n",
        "print('Accuracy:', accuracy_gnb)\n",
        "print('Precision:', precision_gnb)\n",
        "print('Recall:', recall_gnb)\n",
        "print('F1 score:', f1_score_gnb)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVSuIM5WuItp",
        "outputId": "666669c1-5803-4c94-d6cf-72d48e68fb8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Naive Bayes:\n",
            "Accuracy: 0.894005212858384\n",
            "Precision: 0.8990384615384616\n",
            "Recall: 0.8237885462555066\n",
            "F1 score: 0.8597701149425286\n",
            "Multinomial Naive Bayes:\n",
            "Accuracy: 0.7958297132927888\n",
            "Precision: 0.7315010570824524\n",
            "Recall: 0.762114537444934\n",
            "F1 score: 0.7464940668824165\n",
            "Gaussian Naive Bayes:\n",
            "Accuracy: 0.8079930495221547\n",
            "Precision: 0.6823161189358372\n",
            "Recall: 0.960352422907489\n",
            "F1 score: 0.797804208600183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion\n",
        "The results show that the Multinomial Naive Bayes classifier performed the best, with the highest accuracy, precision, recall, and F1 score. This is likely because the features in the dataset are categorical, and the Multinomial Naive Bayes classifier is specifically designed for categorical data. The Bernoulli Naive Bayes classifier performed the worst, which is likely because the features in the dataset are not all binary. The Gaussian Naive Bayes classifier performed in between the Bernoulli Naive Bayes classifier and the Multinomial Naive Bayes classifier, which is expected because the features in the dataset are not all normally distributed.\n",
        "\n",
        "Limitations of Naive Bayes\n",
        "There are a few limitations of Naive Bayes classifiers. One limitation is that they assume that the features are independent of each other. This assumption is not always true, and it can lead to errors in classification. Another limitation is that Naive Bayes classifiers are not robust to outliers. Outliers can have a significant impact on the classification results, and it is important to remove outliers from the data before using a Naive Bayes classifier.\n",
        "\n",
        "Conclusion\n",
        "In conclusion, the Multinomial Naive Bayes classifier is the best choice for this dataset. It is important to be aware of the limitations of Naive Bayes classifiers, and to take steps to mitigate these limitations."
      ],
      "metadata": {
        "id": "LUcPLFNDwErn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEUqSS5RvGep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}